{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys,random,math\r\n",
    "from collections import Counter\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "np.random.seed(1)\r\n",
    "random.seed(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data = pd.read_csv('IMDB.csv')\r\n",
    "raw_reviews = list(data['review'])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "for sent in raw_reviews:\r\n",
    "    wordCnt = Counter()\r\n",
    "    for word in sent.split(' '):\r\n",
    "        wordCnt[word] -=1\r\n",
    "vocab = list(set(map(lambda x:x[0],wordCnt.most_common())))\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "word2index = {}\r\n",
    "for i,word in enumerate(vocab):\r\n",
    "    word2index[word]=i\r\n",
    "print(word2index)\r\n",
    "concatinated = list()\r\n",
    "input_dataset = list()\r\n",
    "for sent in raw_reviews:\r\n",
    "    sent_indices = list()\r\n",
    "    for word in sent.split(' '):\r\n",
    "        try:\r\n",
    "            sent_indices.append(word2index[word])\r\n",
    "            concatinated.append(word2index[word])\r\n",
    "        except:\r\n",
    "            \"\"\r\n",
    "        \r\n",
    "    input_dataset.append(sent_indices)\r\n",
    "concatinated = np.array(concatinated)\r\n",
    "random.shuffle(input_dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'say': 0, 'chance': 1, 'movies': 2, 'best': 3, 'characters': 4, 'and': 5, 'avoid': 6, 'far': 7, '/><br': 8, 'of': 9, 'muddled,': 10, 'in': 11, 'art,': 12, 'known': 13, 'movies.': 14, 'had': 15, 'channels': 16, 'worst': 17, 'who': 18, '-': 19, 'expect': 20, 'would': 21, \"you'll\": 22, 'rental,': 23, 'do': 24, 'Yosemite.<br': 25, 'see': 26, 'implausible': 27, 'left': 28, 'a': 29, 'McCoy': 30, 'only': 31, 'Trek': 32, 'as': 33, 'episodes.': 34, 'movie.': 35, 'just': 36, 'with': 37, 'interact': 38, 'but': 39, 'it': 40, 'me': 41, 'way': 42, 'fans': 43, 'even': 44, '/>I': 45, 'worth': 46, 'all': 47, 'watching,': 48, 'one': 49, 'nine': 50, 'however': 51, 'about': 52, 'be': 53, 'for': 54, 'that': 55, 'Fan': 56, 'watch': 57, 'renting': 58, 'Kirk,': 59, 'hardly': 60, 'goofy': 61, 'not': 62, 'scenes': 63, 'Unfortunately,': 64, 'Spock': 65, \"can't\": 66, 'good': 67, 'save': 68, 'to': 69, 'the': 70, 'movie': 71, 'needs': 72, 'Even': 73, 'movies,': 74, 'high': 75, 'far)': 76, 'Star': 77, 'by': 78, 'cable': 79, 'plot': 80, 'this': 81, 'well': 82, 'another': 83, 'at': 84, '(so': 85, 'including': 86, 'True': 87, 'is': 88, 'expects': 89, 'some': 90, 'No': 91, 'cringing': 92}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "alpha,iterations = 0.05,2\r\n",
    "hidden_size,window,negative = (50,2,5)\r\n",
    "weights_0_1 = (np.random.rand(len(vocab), hidden_size) - 0.5) * 0.2\r\n",
    "weights_1_2 = np.random.rand(len(vocab), hidden_size)*0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "layer_2_target = np.zeros(negative+1)\r\n",
    "layer_2_target[0] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def similar(target):\r\n",
    "    target_index = word2index[target]\r\n",
    "    scores = Counter()\r\n",
    "    for word,index in word2index.items():\r\n",
    "        diff = weights_0_1[index]-weights_0_1[target_index]\r\n",
    "        sq_diff = diff**2\r\n",
    "        scores[word] = -math.sqrt(sum(sq_diff))\r\n",
    "    return scores.most_common(10)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def sigmoid(x):\r\n",
    "    return 1/(1 + np.exp(-x))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "for rev_i,review in enumerate(input_dataset*iterations):\r\n",
    "    for target_i in range(len(review)):\r\n",
    "        target_samples = [review[target_i]]+list(concatinated[\r\n",
    "            (np.random.rand(negative)*len(concatinated)).astype('int').tolist()\r\n",
    "            ])\r\n",
    "        left_context = review[max(0, target_i-window):target_i]\r\n",
    "        right_context=review[target_i+1:min(len(review),target_i+window)]\r\n",
    "        layer_1 = np.mean(weights_0_1[left_context+right_context],axis=0)\r\n",
    "        print(layer_1.shape)\r\n",
    "        layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))\r\n",
    "        print(layer_2.shape)\r\n",
    "        layer_2_delta = layer_2 - layer_2_target\r\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])\r\n",
    "        weights_0_1[left_context+right_context] -= layer_1_delta * alpha\r\n",
    "        weights_1_2[target_samples] -= np.outer(layer_2_delta,layer_1)*alpha\r\n",
    "        break\r\n",
    "    break\r\n",
    "\r\n",
    "    if(rev_i % 250 == 0):\r\n",
    "        sys.stdout.write('\\rProgress:'+str(rev_i/float(len(input_dataset)\r\n",
    "                                                * iterations)) + \" \" + str(similar('well')))\r\n",
    "    sys.stdout.write('\\rProgress:'+str(rev_i/float(len(input_dataset)\r\n",
    "                                                * iterations)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50,)\n",
      "(6,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(similar('well'))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('avoid', nan), ('of', nan), ('movies', nan), ('/><br', nan), ('characters', nan), ('and', nan), ('say', nan), ('far', nan), ('best', nan), ('chance', nan)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "interpreter": {
   "hash": "ce90fcae62cdc09553768240cfe068f0f50d8bd5e5b69ab39b3ef7652f49ed7b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}